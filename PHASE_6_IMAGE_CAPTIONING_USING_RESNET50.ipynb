{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karans17s/Practical_Implementation_Of_Deep_learning/blob/main/PHASE_6_IMAGE_CAPTIONING_USING_RESNET50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk5__Dz5LS5j"
      },
      "source": [
        "# Step_1 : Import Library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxWFSjLOLUW2"
      },
      "source": [
        "### 1. PIL ( Python Imaging Library ) handling and processing images files ( also provide funcationality to open , manipulate and converting image formats )\n",
        "\n",
        "### 2. Resize -- resize imgs to a uniform size 224x224 so fed into neural network\n",
        "\n",
        "### 3. ToTensor -- converts the img to num for model understanding\n",
        "\n",
        "### 4. Normalize -- Scales these numbers to make the training stable\n",
        "\n",
        "### 5. compose -- Combines multiple transformations into a single pipeline.\n",
        "\n",
        "### 6. importing pre-trained ResNet Model ( resnet50 ) 50-layer deep residul network commonly used as an encoder for img captioning ( and extract high level features from imgs )\n",
        "\n",
        "### 7. ResNet50_weights -- improves performance since the model has already learned general img features\n",
        "\n",
        "### 8. io -- opens files for reading and writing ( loading text data caption / tokenization seq.)\n",
        "\n",
        "### 9. Unicodedata -- it's very imp for simplify simpple accented characters\n",
        "\n",
        "### 10. re -- ( regular expression module ) pattern matching and cleaning text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjx_gMswKFyR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.models import resnet50,ResNet50_Weights\n",
        "from io import open\n",
        "import unicodedata # for text normalization\n",
        "import re # for cleaning text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lFvODU7LdCY"
      },
      "source": [
        "# Step_2 : Most General Step Check Your usage Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR6Qg3mDKUcd"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device=torch.device(type='cuda', index=0)\n",
        "else:\n",
        "    device=torch.device(type='cpu', index=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGdUBpZOLgic"
      },
      "source": [
        "# Step_3 : Text Processing / Text Normalization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W7z2UWHLnaW"
      },
      "source": [
        "## 3-steps follow\n",
        "\n",
        "## 1. Unicode Normalization\n",
        "\n",
        "### Converts text into a consistent Unicode format and removes diacritical marks\n",
        "\n",
        "## 2. Character Filtering\n",
        "\n",
        "### Removes unwanted characters (e.g., numbers, symbols) while keeping only alphabets and selected punctuation marks.\n",
        "\n",
        "\n",
        "## 3. Whitespace Normalization\n",
        "\n",
        "### Replaces multiple spaces with a single space and trims leading/trailing spaces.\n",
        "\n",
        "## NFD and NFC -- Normalization Form Decomposed , Normalization Form Composed\n",
        "\n",
        "## Mn -- Mark Nonspacing ( identify and remove those combining marks like accents )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIS3tGVJKYKl"
      },
      "outputs": [],
      "source": [
        "def normalizeString(s):\n",
        "    sres=\"\"\n",
        "    for ch in unicodedata.normalize('NFD', s):\n",
        "        if unicodedata.category(ch) != 'Mn':\n",
        "            sres+=ch\n",
        "    sres = re.sub(r\"[^a-zA-Z!?,]+\", r\" \", sres)\n",
        "    return sres.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6ROoNDHLzio"
      },
      "source": [
        "# Step_4 : Data Extraction....."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNHJDdRSKavG",
        "outputId": "3a0647db-efa0-4118-f3d0-3e05b725b322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted to: /content/image/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_path = \"/content/Images.zip\"\n",
        "extract_path = \"/content/image/\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(f\"Files extracted to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oID_SoocKsZw",
        "outputId": "20cc5357-56c7-4592-ed8d-e0258893d67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted to: /content/caption\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_path = \"/content/captions.txt.zip\"\n",
        "extract_path = \"/content/caption\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(f\"Files extracted to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrI9-MDNNEdU"
      },
      "outputs": [],
      "source": [
        "capt_file_path=\"/content/caption/captions.txt\"\n",
        "images_dir_path=\"/content/image/Images/\"\n",
        "data=open(capt_file_path).read().strip().split('\\n')\n",
        "data=data[1:]\n",
        "img_filenames_list=[]\n",
        "captions_list=[]\n",
        "for s in data:\n",
        "    templist=s.lower().split(\",\")\n",
        "    img_path=templist[0]\n",
        "    caption=\",\".join(s for s in templist[1:])\n",
        "    caption=normalizeString(caption)\n",
        "    img_filenames_list.append(img_path)\n",
        "    captions_list.append(caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLBwwQLL4GC"
      },
      "source": [
        "# Step_5 : Vocabulary Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxK_epSJL6Xj"
      },
      "source": [
        "## word2index -- convert words to numbers\n",
        "\n",
        "## index2word -- convert number to words\n",
        "\n",
        "## word2count -- count occurrences of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpTXOLNdL-Up"
      },
      "outputs": [],
      "source": [
        "max_cap_length=73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WTdi-gtNSmw"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self): # Corrected constructor name\n",
        "        self.word2index={'SOS':0, 'EOS':1}\n",
        "        self.index2word={0:'SOS', 1:'EOS'}\n",
        "        self.word2count={} # count of words\n",
        "        self.nwords=2 # count of total unique words\n",
        "    def buildVocab(self,s):\n",
        "        for word in s.split(\" \"):\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word]=self.nwords\n",
        "                self.index2word[self.nwords]=word\n",
        "                self.word2count[word]=1\n",
        "                self.nwords+=1\n",
        "            else:\n",
        "                self.word2count[word]+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXqVoOP5L9MY"
      },
      "source": [
        "## Splits the caption into words.\n",
        "## Adds each word to the vocabulary if it is not already present.\n",
        "##Updates the frequency count of each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqojZv2RKyfW",
        "outputId": "94585706-b039-4505-9da9-27959c4ba2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Length: 8446\n"
          ]
        }
      ],
      "source": [
        "vocab=Vocab()\n",
        "for caption in captions_list:\n",
        "    vocab.buildVocab(caption)\n",
        "print(\"Vocab Length:\",vocab.nwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVYO28T3MAhW"
      },
      "source": [
        "# STEP : 6 = custom data\n",
        "\n",
        "\n",
        "## why use this , handling non-standard data formats , automatic data extraction , text-to-tensor conversion , easy to access data ( _getitem_ )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No6_tqhEMDpy"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,images_dir_path, img_filenames_list, captions_list, vocab, max_cap_length): # Corrected constructor name to __init__\n",
        "        super().__init__()\n",
        "        self.images_dir_path=images_dir_path\n",
        "        self.img_filenames_list=img_filenames_list\n",
        "        self.captions_list=captions_list\n",
        "        self.length=len(self.captions_list)\n",
        "        self.transform=Compose([Resize((224,224), antialias=True), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "        self.vocab=vocab\n",
        "        self.max_cap_length=max_cap_length\n",
        "\n",
        "    def __len__(self): # Corrected method name to __len__\n",
        "        return self.length\n",
        "\n",
        "    def get_input_ids(self, sentence,vocab):\n",
        "        input_ids=[0]*(self.max_cap_length+1)\n",
        "        i=0\n",
        "        for word in sentence.split(\" \"):\n",
        "            input_ids[i]=vocab.word2index[word]\n",
        "            i=i+1\n",
        "\n",
        "        input_ids.insert(0,vocab.word2index['SOS'])\n",
        "        i=i+1\n",
        "        input_ids[i]=vocab.word2index['EOS']\n",
        "\n",
        "        return torch.tensor(input_ids)\n",
        "\n",
        "    def __getitem__(self,idx): # Corrected method name to __getitem__\n",
        "        imgfname,caption=self.img_filenames_list[idx],self.captions_list[idx]\n",
        "\n",
        "        imgfname=self.images_dir_path+imgfname\n",
        "        img=Image.open(imgfname)\n",
        "        img=self.transform(img)\n",
        "\n",
        "        caption=self.get_input_ids(caption,self.vocab)\n",
        "\n",
        "        return img,caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPpmT7WaMFdv"
      },
      "outputs": [],
      "source": [
        "#driver code part 3\n",
        "dataset=CustomDataset(images_dir_path, img_filenames_list, captions_list, vocab, max_cap_length)\n",
        "train_dataset,test_dataset=random_split(dataset,[0.999,0.001])\n",
        "\n",
        "batch_size=64\n",
        "train_dataloader=DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
        "test_dataloader=DataLoader(dataset=test_dataset,batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXEnSsPMH0V"
      },
      "source": [
        "# STEP : 7 = Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmfcnSQMJcD"
      },
      "source": [
        "### The encoder transforms input images into feature vectors that can be used for further processing\n",
        "\n",
        "### forward pass : Converts raw image data into meaningful representations that can be used by the decoder (text generator)\n",
        "\n",
        "### ResNet-50 is a deep convolutional neural network that extracts useful image features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPorKAnVMMnz"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, pretrained_feature_extractor): # Corrected constructor name to __init__\n",
        "        super().__init__()\n",
        "        self.pretrained_feature_extractor=pretrained_feature_extractor\n",
        "\n",
        "    def forward(self,x):\n",
        "        features=self.pretrained_feature_extractor(x)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CbRCkAlML94"
      },
      "source": [
        "### Replaces the fully connected (FC) layer of ResNet-50.\n",
        "\n",
        "###Original ResNet-50 has fc = nn.Linear(2048, 1000), which maps features to 1000 ImageNet classes.\n",
        "\n",
        "###We change it to nn.Linear(2048, 1024), reducing the feature vector size from 2048 to 1024.\n",
        "\n",
        "### The new 1024-dimensional feature vector is better suited for the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHgGf91JMPDe",
        "outputId": "1076fcd4-921a-4330-ee25-98b7859cc839"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:04<00:00, 22.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "#driver code part 4\n",
        "pretrained_feature_extractor=resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "pretrained_feature_extractor.fc=nn.Linear(2048,1024)\n",
        "\n",
        "encoder=Encoder(pretrained_feature_extractor).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3VsqikMPhD"
      },
      "source": [
        "# STEP : 8 = Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKiijvFoMR7K"
      },
      "source": [
        "### output_size: Number of words in the vocabulary.\n",
        "\n",
        "### embed_size: Size of word embeddings (vector representation of words).\n",
        "\n",
        "###hidden_size: Number of hidden units in the GRU (memory for the decoder).\n",
        "\n",
        "### why??? Converts numerical input sequences (word indices) into meaningful text sentences.\n",
        "\n",
        "### Defines a GRU (Gated Recurrent Unit) layer.\n",
        "\n",
        "###Takes word embeddings (embed_size) as input.\n",
        "\n",
        "###Outputs a hidden representation (hidden_size) that captures the context of the sentence.\n",
        "\n",
        "### why?? GRU remembers past words while predicting the next word.\n",
        "\n",
        "### Converts GRU output into probabilities for each word in the vocabulary.\n",
        "\n",
        "### self.lsoftmax=nn.LogSoftmax(dim=-1) -- logsoftmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u7FLmWjMUKY"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_size,embed_size,hidden_size): # Corrected constructor name to __init__\n",
        "        super().__init__()\n",
        "        self.e=nn.Embedding(output_size,embed_size)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.gru=nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.lin=nn.Linear(hidden_size,output_size)\n",
        "        self.lsoftmax=nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self,x,prev_hidden):\n",
        "        x=self.e(x)\n",
        "        x=self.relu(x)\n",
        "        output,hidden=self.gru(x,prev_hidden)\n",
        "        y=self.lin(output)\n",
        "        y=self.lsoftmax(y)\n",
        "        return y, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEgQQDR5MUe8"
      },
      "outputs": [],
      "source": [
        "#driver code part 5\n",
        "embed_size=300\n",
        "hidden_size=1024\n",
        "\n",
        "decoder=Decoder(vocab.nwords,embed_size,hidden_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__CLSBA6MUxb"
      },
      "source": [
        "# STEP : 9 = training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyD2Grp8MXDS"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch():\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    track_loss=0\n",
        "\n",
        "    for i, (imgs,t_ids) in enumerate(train_dataloader):\n",
        "        imgs=imgs.to(device)\n",
        "        t_ids=t_ids.to(device)\n",
        "        extracted_features=encoder(imgs)\n",
        "        #extracted_features=extracted_features.detach()\n",
        "        decoder_hidden=torch.reshape(extracted_features,(1,extracted_features.shape[0],-1))\n",
        "        yhats, decoder_hidden = decoder(t_ids[:,0:-1],decoder_hidden)\n",
        "\n",
        "        gt=t_ids[:,1:]\n",
        "\n",
        "        yhats_reshaped=yhats.view(-1,yhats.shape[-1])\n",
        "\n",
        "        gt=gt.reshape(-1)\n",
        "\n",
        "\n",
        "        loss=loss_fn(yhats_reshaped,gt)\n",
        "        track_loss+=loss.item()\n",
        "\n",
        "        opte.zero_grad()\n",
        "        optd.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        opte.step()\n",
        "        optd.step()\n",
        "\n",
        "        if i%50==0:\n",
        "            print(\"Mini Batch=\", i+1,\" Running Loss=\",track_loss/(i+1), sep=\"\")\n",
        "\n",
        "    return track_loss/len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmZbXhB_MXR5"
      },
      "outputs": [],
      "source": [
        "def ids2Sentence(ids,vocab):\n",
        "    sentence=\"\"\n",
        "    for id in ids.squeeze():\n",
        "        if id==0:\n",
        "            continue\n",
        "        word=vocab.index2word[id.item()]\n",
        "        sentence+=word + \" \"\n",
        "        if id==1:\n",
        "            break\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsegSnVtMXml"
      },
      "source": [
        "# STEP : 10 = Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHMxXP_7OXou"
      },
      "outputs": [],
      "source": [
        "#eval loop (written assuming batch_size=1)\n",
        "def eval_one_epoch():\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    track_loss=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (imgs,t_ids) in enumerate(test_dataloader):\n",
        "\n",
        "            imgs=imgs.to(device)\n",
        "            t_ids=t_ids.to(device)\n",
        "\n",
        "            extracted_features=encoder(imgs)\n",
        "\n",
        "            decoder_hidden=torch.reshape(extracted_features,(1,extracted_features.shape[0],-1)) #n_dim=3\n",
        "\n",
        "            input_ids=t_ids[:,0]\n",
        "            yhats=[]\n",
        "            pred_sentence=\"\"\n",
        "\n",
        "            for j in range(1,max_cap_length+2): #j starts from 1\n",
        "                probs, decoder_hidden = decoder(input_ids.unsqueeze(1),decoder_hidden)\n",
        "                yhats.append(probs)\n",
        "                _,input_ids=torch.topk(probs,1,dim=-1)\n",
        "                input_ids=input_ids.squeeze(1,2) #still a tensor\n",
        "                word=vocab.index2word[input_ids.item()] #batch_size=1\n",
        "                pred_sentence+=word + \" \"\n",
        "                if input_ids.item() == 1: #batch_size=1\n",
        "                    break\n",
        "\n",
        "\n",
        "            gt_sentence=ids2Sentence(t_ids,vocab)\n",
        "\n",
        "            print(\"Input Image:\")\n",
        "            img=imgs[0]\n",
        "            img[0]=(img[0]*0.229)+0.485\n",
        "            img[1]=(img[1]*0.224)+0.456\n",
        "            img[2]=(img[2]*0.225)+0.406\n",
        "            plt.imshow(torch.permute(imgs[0],(1,2,0)).detach().cpu())\n",
        "            plt.show()\n",
        "\n",
        "            print(\"GT Sentence:\",gt_sentence)\n",
        "\n",
        "            print(\"Predicted Sentence:\",pred_sentence)\n",
        "\n",
        "            yhats_cat=torch.cat(yhats,dim=1)\n",
        "            yhats_reshaped=yhats_cat.view(-1,yhats_cat.shape[-1])\n",
        "            gt=t_ids[:,1:j+1]\n",
        "            gt=gt.view(-1)\n",
        "\n",
        "\n",
        "            loss=loss_fn(yhats_reshaped,gt)\n",
        "            track_loss+=loss.item()\n",
        "\n",
        "\n",
        "        print(\"-----------------------------------\")\n",
        "        return track_loss/len(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJK6jLydOaCZ",
        "outputId": "8b36d65b-8e21-4eaf-e35d-739d5e0fc34a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mini Batch=1 Running Loss=9.064282417297363\n",
            "Mini Batch=51 Running Loss=5.202331524269254\n",
            "Mini Batch=101 Running Loss=4.7045413031436425\n",
            "Mini Batch=151 Running Loss=4.39785737075553\n",
            "Mini Batch=201 Running Loss=4.199654518668331\n",
            "Mini Batch=251 Running Loss=4.051222480150808\n",
            "Mini Batch=301 Running Loss=3.9371926396392114\n",
            "Mini Batch=351 Running Loss=3.844473129663712\n",
            "Mini Batch=401 Running Loss=3.7623244865874104\n",
            "Mini Batch=451 Running Loss=3.6928043645659994\n",
            "Mini Batch=501 Running Loss=3.6376399156338204\n"
          ]
        }
      ],
      "source": [
        "#driver code part 5\n",
        "\n",
        "loss_fn=nn.NLLLoss(ignore_index=0).to(device)\n",
        "lr=0.001\n",
        "\n",
        "optd=optim.Adam(params=decoder.parameters(), lr=lr)\n",
        "opte=optim.Adam(params=encoder.parameters(), lr=lr)\n",
        "\n",
        "n_epochs=5\n",
        "\n",
        "for e in range(n_epochs):\n",
        "    print(\"Epoch=\",e+1, \" Loss=\", round(train_one_epoch(),4), sep=\"\")\n",
        "\n",
        "for e in range(1):\n",
        "    print(\"Epoch=\",e+1, \" Loss=\", round(eval_one_epoch(),4), sep=\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6O/awaQYxB40KmtW2h8P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}